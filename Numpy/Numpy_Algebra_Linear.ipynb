{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algebra Linear con Numpy\n",
    "\n",
    "Uno de las ramas mas comunes en las matematicas es el *Algebra Linear* la cual se enfoca en ecuaciones y el mapeo de espacios lineales aka espacios vectoriales. En computer science vamos a utilizar mucho el algebra linear cuando trabajemos con Machine Learning, en estos proyectos vamos a estar trabajando con matrices de alta dimension las cuales podemos transformar en ecuaciones lineares con las que podemos comparar las relaciones entre los elementos de las distintas matrices.\n",
    "\n",
    "Vamos a suponer el ejemplo anterior en donde queremos localizar a un ladron en una ciudad, anteriormente esto se lograria dandole una foto del sospechoso a todos los policias y ellos tratarian de encontrarlo en toda la ciudad haciendo de esto una tarea MUY ineficiente. Para que un algoritmo pueda hacer una tarea similar tenemos que pensar en las tareas que se deben cumplir en el mas alto detalle posible para que la maquina pueda entender la tarea que queremos que haga.\n",
    "\n",
    "Supongamos que la imagen del ladron es de 512 * 512, cada pixel seria un punto en 262,144 en pixeles aka **son un chingo** por lo que va a ser necesario utilizar algebra linear para poder manipular cualquiera de los pixeles dentro de la matriz. Por si las dudas veamos un ejemplo sencillo de como arreglariamos esto con Deep Learning. Una manera muy sencilla de entender que es esto es pensar en un algoritmo que usa una red neuronal artificial (ANN) para aprender el output que queremos mediante ajustes a la ponderacion que conectan las capas, ejemplo:\n",
    "\n",
    "\n",
    "<img src='../data/img/neuralnetwork.png'>\n",
    "\n",
    "Las redes neuronales almacenan pesos entre capas y valores de sesgo en matrices. Como estos son los parámetros que intenta sintonizar con nuestro modelo de Deep Learning para minimizar el loss function, continuamente realiza cálculos y los actualiza. En general, los modelos Machine Learning requieren cálculos complejos y necesitan ser entrenados con datasets grandes para proporcionar resultados eficientes. Esta es la razon por la cual el algebra linear es importante cuando hablamos de Machine Learning.\n",
    "\n",
    "## Matemáticas vectoriales y matriciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "a = np.arange(6).reshape(3,2) \n",
    "b = np.arange(10).reshape(2,5) \n",
    "print(a) \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  6,  7,  8,  9],\n",
       "       [15, 20, 25, 30, 35],\n",
       "       [25, 34, 43, 52, 61]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  6,  7,  8,  9],\n",
       "       [15, 20, 25, 30, 35],\n",
       "       [25, 34, 43, 52, 61]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  6,  7,  8,  9],\n",
       "       [15, 20, 25, 30, 35],\n",
       "       [25, 34, 43, 52, 61]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a@b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvamos a nuestro ejemplo del ladron, tenemos a 3 policias siguiendo a 3 sospechosos en distintas partes de la ciudad, una vez que los detienen los van a comparar con el retrato hablado del ladron, esto es ineficiente y poco practico, la pregunta ahora es como un algoritmo podria decidir a cual de los sospechosos deben llevar a la carcel? \n",
    "\n",
    "Veamos una representacion geometrica de como el algoritmo platearia este problema. \n",
    "\n",
    "<img src='../data/img/geometric.png'>\n",
    "\n",
    "Como puedes ver esto es un ejemplo muy simple de como el algoritmo va a mostrar la similitud entre ambos vectores, en este caso si la direccion de uno de los sospechosos se acerca al punto 'A', lo va a arrestar. Si esta en la direccion de 'B' lo dejara libre.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1700,  1855,  2010,  2165,  2320],\n",
       "       [ 5300,  5770,  6240,  6710,  7180],\n",
       "       [ 8900,  9685, 10470, 11255, 12040],\n",
       "       [12500, 13600, 14700, 15800, 16900]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# si queremos multiplicar 2 o mas matrices en una sola funcion usar 'linalg.multi_dot()' es conveniente\n",
    "from numpy.linalg import multi_dot \n",
    "a = np.arange(12).reshape(4,3) \n",
    "b = np.arange(15).reshape(3,5) \n",
    "c = np.arange(25).reshape(5,5)\n",
    "multi_dot([a, b, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1700,  1855,  2010,  2165,  2320],\n",
       "       [ 5300,  5770,  6240,  6710,  7180],\n",
       "       [ 8900,  9685, 10470, 11255, 12040],\n",
       "       [12500, 13600, 14700, 15800, 16900]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lo mismo pero usando 'np.dot()', con el inconveniente que debes saber cual es el orden de ejecucion\n",
    "# que tomara menos tiempo\n",
    "a.dot(b).dot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi_dot tarda 0.17628788948059082 segundos.\n",
      "Chain dot tarda 0.2081441879272461 segundos.\n"
     ]
    }
   ],
   "source": [
    "# veamos una comparacion de tiempo entre los dos metodos\n",
    "import numpy as np\n",
    "from numpy.linalg import multi_dot\n",
    "import time\n",
    "a = np.arange(120000).reshape(400,300) \n",
    "b = np.arange(150000).reshape(300,500) \n",
    "c = np.arange(200000).reshape(500,400)\n",
    "start = time.time() \n",
    "multi_dot([a,b,c]) \n",
    "ft = time.time()-start \n",
    "print ('Multi_dot tarda', time.time()-start,'segundos.') \n",
    "start_ft = time.time()\n",
    "a.dot(b).dot(c) \n",
    "print ('Chain dot tarda', time.time()-start_ft,'segundos.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar el metodo *multi_dot()* reduce el tiempo de ejecucion aun con matrices chicas, este gap o diferencia incrementa con en aumento de dimensiones y cantidad de matrices.\n",
    "\n",
    "De igual manera los metodos *outer()* y *inner()*. El metodo outer calcula el producto externo de dos vectores mientras que el metodo inner se comporta de manera diferente dependiendo de los argumentos que toma. Si tiene dos vectores como argumentos, produce un producto *dot* ordinario, pero cuando tiene una matriz dimensional más alta, devuelve la suma del producto sobre los últimos ejes. Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(9).reshape(3,3) \n",
    "b = np.arange(3) \n",
    "print(a) \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 14, 23])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [ 0,  1,  2],\n",
       "       [ 0,  2,  4],\n",
       "       [ 0,  3,  6],\n",
       "       [ 0,  4,  8],\n",
       "       [ 0,  5, 10],\n",
       "       [ 0,  6, 12],\n",
       "       [ 0,  7, 14],\n",
       "       [ 0,  8, 16]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outer(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(9) \n",
    "np.ndim(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [ 0,  1,  2],\n",
       "       [ 0,  2,  4],\n",
       "       [ 0,  3,  6],\n",
       "       [ 0,  4,  8],\n",
       "       [ 0,  5, 10],\n",
       "       [ 0,  6, 12],\n",
       "       [ 0,  7, 14],\n",
       "       [ 0,  8, 16]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outer(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]]\n",
      "\n",
      " [[ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]]\n",
      "[[[ 0  1  2  3  4  5  6  7]\n",
      "  [ 8  9 10 11 12 13 14 15]]\n",
      "\n",
      " [[16 17 18 19 20 21 22 23]\n",
      "  [24 25 26 27 28 29 30 31]]\n",
      "\n",
      " [[32 33 34 35 36 37 38 39]\n",
      "  [40 41 42 43 44 45 46 47]]]\n"
     ]
    }
   ],
   "source": [
    "# ahora veamos lo que pasa cuando usamos el metodo 'tensordot()'\n",
    "a = np.arange(12).reshape(2,3,2) \n",
    "b = np.arange(48).reshape(3,2,8) \n",
    "c = np.tensordot(a,b, axes =([1,0],[0,1])) \n",
    "print(a) \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 800,  830,  860,  890,  920,  950,  980, 1010],\n",
       "       [ 920,  956,  992, 1028, 1064, 1100, 1136, 1172]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valores propios o eigenvalues\n",
    "\n",
    "Un valor propio es un coeficiente de un vector propio/autovectores. Por definición, un vector propio es un vector distinto de cero que solo cambia por un factor escalar cuando se aplica una transformación lineal. En general, cuando la transformación lineal se aplica a un vector, su tramo (la línea que pasa por su origen) se desplaza, pero algunos vectores especiales no se ven afectados por estas transformaciones lineales y permanecen en su propio tramo. Estos son lo que llamamos vectores propios. La transformación lineal los afecta solo estirándolos o aplastándolos a medida que multiplica este vector con un escalar. El valor de este escalar se denomina valor propio.\n",
    "\n",
    "\n",
    "En los algoritmos ML, trabajamos con grandes cantidades de dimensiones. El problema principal no es tener una dimensión enorme, sino la compatibilidad y el rendimiento de su algoritmo con ellos. Por ejemplo, en PCA(principal component analysis), intentamos descubrir la combinación lineal más significativa de su dimensión. La idea principal en PCA es reducir la dimensión de su conjunto de datos y minimizar la pérdida de información. La pérdida de información aquí es en realidad una variación de sus features/características. Un ejemplo:\n",
    "\n",
    "|Feature 1|Feature 2|Feature 3|Feature 4|Feature 5|Clase|\n",
    "|---------|---------|---------|---------|---------|---------|\n",
    "|1.45|42|54|1.001|1.05|Perro|\n",
    "|2|12|34|1.004|24.1|Gato|\n",
    "|4|54|10|1.004|13.4|Perro|\n",
    "|1.2|31|1|1.003|42.1|Gato|\n",
    "|5|4|41|1.003|41.4|Perro|\n",
    "\n",
    "## Que caracteristicas son relevantes? \n",
    "\n",
    "En la tabla anterior la Característica 4 en realidad no hace una diferencia significativa si la clase es Perro o Gato. Esta característica será redundante en nuestro análisis. El objetivo principal aquí es mantener las características que difieren fuertemente entre las clases, por lo que el valor de la característica juega un papel importante en nuestra decision decisión. \n",
    "\n",
    "Y ahora un ejemplo con codigo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primero importamos los datasets y luego tenemos que estandarizar los datos, la estandarizacion es muy importante\n",
    "# y en ocasiones indispensable. En este caso vamos a utilizar el metodo 'StandardScaler()' para estandarizar las\n",
    "# caracteristicas el propósito principal aquí es tener características como en los datos estándar normalmente\n",
    "# distribuidos (media = 0 y variación de la unidad). El metodo 'fit_transform()' lo usamos para \n",
    "# transformar los datos originales en una forma en que la distribución tendrá un valor medio 0 \n",
    "# y una desviación estándar 1. Calculará los parámetros necesarios y aplicará la transformación. \n",
    "# Como usamos StandardScaler (), estos parámetros serán y. Tenga en cuenta que la estandarización no produce\n",
    "# datos distribuidos normalmente de nuestro conjunto de datos original. Se acaba de volver a escalar los datos \n",
    "# donde tenemos una media de cero y una desviación estándar de uno\n",
    "import numpy as np\n",
    "from sklearn import decomposition, datasets\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "data = datasets.load_breast_cancer() \n",
    "cancer = data.data \n",
    "cancer = StandardScaler().fit_transform(cancer) \n",
    "cancer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.99],\n",
       "       [20.57],\n",
       "       [19.69],\n",
       "       [11.42],\n",
       "       [20.29],\n",
       "       [12.45],\n",
       "       [18.25],\n",
       "       [13.71],\n",
       "       [13.  ],\n",
       "       [12.46]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_transformation = data.data \n",
    "before_transformation[:10,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09706398],\n",
       "       [ 1.82982061],\n",
       "       [ 1.57988811],\n",
       "       [-0.76890929],\n",
       "       [ 1.75029663],\n",
       "       [-0.47637467],\n",
       "       [ 1.17090767],\n",
       "       [-0.11851678],\n",
       "       [-0.32016686],\n",
       "       [-0.47353452]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformamos los datos a una version estandarizada\n",
    "cancer[:10,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# despues de transformar los datos, calcularemos la matriz de covarianza; para calcular el valor propio \n",
    "# y el vector propio con el método 'np.linalg.eig()' y luego utilizarlos en descomposicion.\n",
    "covariance_matrix = np.cov(cancer,rowvar=False)\n",
    "covariance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val_cov, eig_vec_cov = np.linalg.eig(covariance_matrix)\n",
    "eig_pairs = [(np.abs(eig_val_cov[i]), eig_vec_cov[:,i]) for i in\n",
    "range(len(eig_val_cov))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.304990794374554\n",
      "5.701374603726142\n",
      "2.822910155006227\n",
      "1.9841275177301991\n",
      "1.6516332423301197\n",
      "1.2094822398029734\n",
      "0.6764088817009056\n",
      "0.4774562546895088\n",
      "0.41762878210781545\n",
      "0.35131087488173374\n",
      "0.2944331534911646\n",
      "0.26162116136612124\n",
      "0.2417824213283134\n",
      "0.15728614921759335\n",
      "0.0943006956010559\n",
      "0.08000340447737649\n",
      "0.05950361353043175\n",
      "0.05271142221014807\n",
      "0.04956470021298171\n",
      "0.031214260553066475\n",
      "0.030025663090428662\n",
      "0.027487711338904257\n",
      "0.024383691354591005\n",
      "0.01808679398430557\n",
      "0.01550852713441875\n",
      "0.008192037117606769\n",
      "0.006912612579184394\n",
      "0.0015921360011975276\n",
      "0.0007501214127190591\n",
      "0.00013327905666360806\n"
     ]
    }
   ],
   "source": [
    "# como podemos veren el codigo anterior, calculamos la matriz de covarianza construida para todas \n",
    "# las características. Como tenemos 30 características en el dataset, la matriz de covarianza tiene \n",
    "# una forma de (30,30) matriz 2-D. El siguiente bloque de código ordena los valores propios en orden descendente\n",
    "sorted_pairs = eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.19283683, 1.94858307])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tenemos que ordenar los valores propios en orden decreciente para decidir qué vectores propios deseamos eliminar\n",
    "# para reducir el espacio de trabajo dimensional. Como podemos ver en la lista ordenada anterior, los dos primeros\n",
    "# vectores propios con valores propios altos son los que más información tienen sobre la distribución de los datos;\n",
    "# por lo tanto, el resto se eliminará para el subespacio dimensional inferior.\n",
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(30,1), eig_pairs[1][1].reshape(30,1)))\n",
    "matrix_w.shape \n",
    "transformed = matrix_w.T.dot(cancer.T) \n",
    "transformed = transformed.T \n",
    "transformed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.19283683, 1.94858307])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en el bloque de código anterior, los dos primeros vectores propios se apilan horizontalmente, ya que se \n",
    "# utilizarán en la multiplicación de matrices para transformar nuestros datos para las nuevas dimensiones en \n",
    "# el nuevo subespacio. Los datos finales se transformaron de (569,30) a (569,2) matriz, lo que significa que \n",
    "# 28 características se eliminaron durante el proceso de PCA\n",
    "import numpy as np \n",
    "from sklearn import decomposition \n",
    "from sklearn import datasets \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "pca = decomposition.PCA(n_components=2) \n",
    "x_std = StandardScaler().fit_transform(cancer)\n",
    "pca.fit_transform(x_std)[0]\n",
    "\n",
    "# por otro lado, hay funciones integradas en otras bibliotecas que realizan las mismas operaciones que usted \n",
    "# realiza. En scikit-learn, hay muchos métodos incorporados que puede utilizar para los algoritmos ML. Como podemos\n",
    "# ver en el bloque de código anterior, la misma PCA se realizó mediante tres líneas de código con dos métodos. \n",
    "# Sin embargo, la intención de este ejemplo es mostrarle la importancia de los valores propios y los vectores \n",
    "# propios, por lo tanto, el libro muestra el sencillo camino de la PCA con NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo de la norma y determinante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
